{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'bool': {'filter': [{'range': {'@timestamp': {'gte': '2018-10-27T00:00:00.000Z', 'lt': '2018-10-27T23:59:59.999Z'}}}]}}}\n",
      "Filtered logs: 4723499\n",
      "Total number of logs: 4723499\n"
     ]
    }
   ],
   "source": [
    "import elasticsearch\n",
    "import unicodedata\n",
    "import csv\n",
    "from elasticsearch_dsl.aggs import Terms, Nested\n",
    "from elasticsearch_dsl.search import Search\n",
    "from elasticsearch_dsl import Q\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "from pandasticsearch import Select\n",
    "import numpy as np\n",
    "\n",
    "es = elasticsearch.Elasticsearch([\"149.165.170.209:9200\"])\n",
    "#res = Search(using=es, index=\"filebeat*\").query(\"match\", username=\"root\")\n",
    "\n",
    "#####################################################################\n",
    "# Code for quering between dates (Not Working)\n",
    "#####################################################################\n",
    "# field_query = Q('query_string', query='_exists_:ept.runtime_seconds')\n",
    "# time_query = Q('range',**{\"@timestamp\":{\n",
    "#     'from': '2018-10-24T00:00:00.000Z',\n",
    "#     'to': '2018-10-24T23:59:59.999Z'\n",
    "# }})\n",
    "# combined = Q('bool', filter=[field_query, time_query])\n",
    "# res = Search(using=es, index=\"packetbeat*\")\n",
    "# response = res.query(combined)\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# Make your Query with elasticsearch_dsl\n",
    "#####################################################################\n",
    "#res = Search(using=es, index=\"packetbeat*\").filter('range',**{\"@timestamp\":{'from': datetime.datetime.now().strftime('2018-10-23T00:00:00.000Z'), 'to' : datetime.datetime.now().strf\n",
    "#time('2019-01-31T23:59:59.999Z')}})\n",
    "res = Search(using=es, index=\"packetbeat*\").filter('range',**{\"@timestamp\":{'gte': '2018-10-27T00:00:00.000Z', 'lt' : '2018-10-27T23:59:59.999Z'}}).params(request_timeout=10000)#.sort(\"timestamp\", {'order': \"desc\"})\n",
    "#res.aggs.bucket('properties', Nested(path='properties')).bucket('hostname', Terms(field='host.name.keyword'))\n",
    "# res.sort('@timestamp')\n",
    "#####################################################################\n",
    "# Print the underlying JSON representation of your Query\n",
    "#####################################################################\n",
    "res_json = res.to_dict()\n",
    "print(res_json)\n",
    "    \n",
    "#####################################################################\n",
    "# Syntax for filtering data\n",
    "#####################################################################\n",
    "#        .query('bool', filter=Q('exists', field='ip') & Q('exists', field='port')\\\n",
    "#               & Q('exists', field='client_ip') & Q('exists', field='client_port')\\\n",
    "#               & Q('exists', field='type') & Q('exists', field='method') & Q('exists', field='status')\\\n",
    "#               & Q('exists', field='bytes_in') & Q('exists', field='bytes_out')\n",
    "#              )\n",
    "# Code for quering between dates (Not Working)\n",
    "#####################################################################\n",
    "\n",
    "#    .query(\"match\", ip=\"/[0-9]{2,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}/\")\n",
    "\n",
    "response = res.execute()\n",
    "#filtered=res.filter(\"exists\", field='ip' & \"exists\", field='client_ip').count()\n",
    "#print(filtered)\n",
    "\n",
    "print(\"Filtered logs: %i\" %res.count()) #There is an error on the number of matched logs\n",
    "print(\"Total number of logs: %i\" %response.hits.total)\n",
    "\n",
    "#h = response.hits[0]\n",
    "#print('/%s/%s/%s returned with score %f' % (\n",
    "#    h.meta.index, h.meta.doc_type, h.meta.id, h.meta.score))\n",
    "\n",
    "#df = pd.DataFrame((d.to_dict() for d in res))\n",
    "###print(res['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Make a small test for normalizing a json response (single flow) from elasticsearch\n",
    "#####################################################################\n",
    "from pandas.io.json import json_normalize\n",
    "my_json = {'flow_id': 'EQQA////DP//////FP8BAAH6Fj4hHCz6Fj5kj2kKAQFBCgEBGLQTxuw', 'host': {'name': 'elk-master'}, 'start_time': '2018-10-23T23:47:34.605Z', '@timestamp': '2018-10-23T23:48:20.193Z', 'type': 'flow', 'source': {'ip': '10.1.1.24', 'port': 60614, 'stats': {'net_bytes_total': 14750, 'net_packets_total': 48}, 'mac': 'fa:16:3e:64:8f:69'}, 'final': False, 'tags': ['beats_input_raw_event'], 'dest': {'ip': '10.1.1.65', 'port': 5044, 'stats': {'net_bytes_total': 2424, 'net_packets_total': 24}, 'mac': 'fa:16:3e:21:1c:2c'}, 'beat': {'hostname': 'elk-master', 'name': 'elk-master', 'version': '6.4.2'}, 'transport': 'tcp', '@version': '1', 'last_time': '2018-10-23T23:48:17.797Z'}\n",
    "pandas_df = json_normalize(my_json)\n",
    "#pandas_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-30 03:38:35.459101\n"
     ]
    }
   ],
   "source": [
    "pandas_df.head()\n",
    "print(datetime.datetime.now())\n",
    "# n_rows = len(pandas_df.index)\n",
    "# if (n_rows % 100000) == 0:\n",
    "#     print(n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "2019-06-30 03:38:37.249955\n",
      "#####################\n",
      "2019-06-30 03:39:24.541809\n",
      "2500\n",
      "\n",
      "\n",
      "2019-06-30 03:40:15.112595\n",
      "5000\n",
      "\n",
      "\n",
      "2019-06-30 03:41:24.548983\n",
      "7500\n",
      "\n",
      "\n",
      "2019-06-30 03:42:51.426345\n",
      "10000\n",
      "\n",
      "\n",
      "2019-06-30 03:44:37.141480\n",
      "12500\n",
      "\n",
      "\n",
      "2019-06-30 03:46:43.380291\n",
      "15000\n",
      "\n",
      "\n",
      "2019-06-30 03:49:12.275193\n",
      "17500\n",
      "\n",
      "\n",
      "2019-06-30 03:52:03.940104\n",
      "20000\n",
      "\n",
      "\n",
      "2019-06-30 03:55:20.210402\n",
      "22500\n",
      "\n",
      "\n",
      "2019-06-30 03:59:05.069492\n",
      "25000\n",
      "\n",
      "\n",
      "2019-06-30 04:03:16.338932\n",
      "27500\n",
      "\n",
      "\n",
      "2019-06-30 04:07:50.617647\n",
      "30000\n",
      "\n",
      "\n",
      "2019-06-30 04:12:50.603592\n",
      "32500\n",
      "\n",
      "\n",
      "2019-06-30 04:18:14.327096\n",
      "35000\n",
      "\n",
      "\n",
      "2019-06-30 04:24:01.872780\n",
      "37500\n",
      "\n",
      "\n",
      "2019-06-30 04:30:12.366791\n",
      "40000\n",
      "\n",
      "\n",
      "2019-06-30 04:36:48.170087\n",
      "42500\n",
      "\n",
      "\n",
      "2019-06-30 04:43:48.383488\n",
      "45000\n",
      "\n",
      "\n",
      "2019-06-30 04:51:14.348073\n",
      "47500\n",
      "\n",
      "\n",
      "2019-06-30 04:59:09.430513\n",
      "50000\n",
      "\n",
      "\n",
      "2019-06-30 05:07:31.970271\n",
      "52500\n",
      "\n",
      "\n",
      "2019-06-30 05:16:21.459223\n",
      "55000\n",
      "\n",
      "\n",
      "2019-06-30 05:25:41.806746\n",
      "57500\n",
      "\n",
      "\n",
      "2019-06-30 05:35:30.184269\n",
      "60000\n",
      "\n",
      "\n",
      "2019-06-30 05:45:49.737649\n",
      "62500\n",
      "\n",
      "\n",
      "2019-06-30 05:56:36.307933\n",
      "65000\n",
      "\n",
      "\n",
      "2019-06-30 06:07:53.653859\n",
      "67500\n",
      "\n",
      "\n",
      "2019-06-30 06:19:42.060897\n",
      "70000\n",
      "\n",
      "\n",
      "2019-06-30 06:32:03.740707\n",
      "72500\n",
      "\n",
      "\n",
      "2019-06-30 06:44:55.918324\n",
      "75000\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ScanError",
     "evalue": "Scroll request has only succeeded on 4 shards out of 5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScanError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6e39987188cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#####################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mhits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#main_pandas_df = pd.concat([main_pandas_df,temp_df],axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/elasticsearch_dsl/search.py\u001b[0m in \u001b[0;36mscan\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         ):\n\u001b[1;32m    721\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/elasticsearch/helpers/actions.py\u001b[0m in \u001b[0;36mscan\u001b[0;34m(client, query, scroll, raise_on_error, preserve_order, size, request_timeout, clear_scroll, scroll_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0mscroll_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                         \u001b[0;34m\"Scroll request has only succeeded on %d shards out of %d.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                         \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_shards\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"successful\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_shards\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                     )\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mScanError\u001b[0m: Scroll request has only succeeded on 4 shards out of 5."
     ]
    }
   ],
   "source": [
    "#pandas_df = Select.from_dict(res.scan()).to_pandas()\n",
    "#df = pd.concat(map(pd.DataFrame.from_dict, res.scan()), axis=1)['fields'].T\n",
    "# from pandas.io.json import json_normalize\n",
    "# df = json_normalize(res['hits']['hits'])\n",
    "\n",
    "\n",
    "# for hits in res.scan():\n",
    "#     print(hits.to_dict())\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# for hits in res.scan():\n",
    "#     pandas_df2 = json_normalize(hits.to_dict())\n",
    "\n",
    "# pandas_df2.head\n",
    "# print(res.scan()['hits']['hits'])\n",
    "\n",
    "#####################################################################\n",
    "# Create a dummy dataframe to which we will append flows from Elasticsearch\n",
    "#####################################################################\n",
    "todays_date = datetime.datetime.now().date()\n",
    "index = index = pd.date_range(todays_date-datetime.timedelta(1), periods=1, freq='D')\n",
    "# index = 0\n",
    "data = np.array([np.arange(1)]*3).T\n",
    "# data = ['dummy','dummy','dummy']\n",
    "columns = ['@timestamp','beat.hostname', 'beat.version']\n",
    "main_pandas_df = pd.DataFrame(data, index=index, columns=columns)\n",
    "\n",
    "#pandas_df = Select.from_dict(x for hit in res.scan()).to_pandas()\n",
    "#main_pandas_df\n",
    "\n",
    "#####################################################################\n",
    "# CNormalize each response (flow in response) and append to dataframe\n",
    "#####################################################################\n",
    "print(\"#####################\")\n",
    "print(datetime.datetime.now())\n",
    "print(\"#####################\")\n",
    "for hits in res.scan():\n",
    "    temp_df = json_normalize(hits.to_dict())\n",
    "    #main_pandas_df = pd.concat([main_pandas_df,temp_df],axis=1)\n",
    "    main_pandas_df = main_pandas_df.append(temp_df, ignore_index=True, sort=False)\n",
    "    n_rows = len(main_pandas_df.index)\n",
    "    if (n_rows % 2500) == 0:\n",
    "        print(datetime.datetime.now())\n",
    "        print(n_rows)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "#####################################################################\n",
    "# Visualize the results\n",
    "#####################################################################\n",
    "#main_pandas_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Do NOT Delete - Extracting specific fields from ES response\n",
    "######################################################################\n",
    "\n",
    "# df_tcp_udp = pd.DataFrame(((hit[\"@timestamp\"],hit['host']['name'],hit['type'],hit['method'],hit['path'],hit['direction'],hit['status'],hit['ip'],hit['port'],hit['client_ip'],hit['client_port'],hit['bytes_in'],hit['bytes_out'],hit['responsetime'],hit['http.request.headers.content-length'],hit['http.request.params'],hit['http.request.code'],hit['http.response.headers.content-length'],hit['http.response.headers.content-type'],hit['http.response.phrase'], hit['dest.stats.net_bytes_total'],hit['dest.stats.net_packets_total'],hit['source.stats.net_bytes_total'],hit['source.stats.net_packets_total'],hit['final']) for hit in res.scan()),\\\n",
    "#                     columns=['Timestamp','hostname','type','method','path','direction','status','ip','port','client_ip','client_port','bytes_in','bytes_out','responsetime','http.request.headers.content-length','http.request.params','http.response.code','http.response.headers.content-length','http.response.headers.content-type','http.response.phrase','dest.stats.net_bytes_total','dest.stats.net_packets_total','source.stats.net_bytes_total','source.stats.net_packets_total','final'])\n",
    "\n",
    "######################################################################\n",
    "# Sorting by timestamp using this approach did not worked\n",
    "######################################################################\n",
    "# Convert timestamp to datetime\n",
    "main_pandas_df['@timestamp'] = pd.to_datetime(main_pandas_df['@timestamp'])\n",
    "# Check the number of fields and the data type for each field\n",
    "#print(main_pandas_df.info())                        \n",
    "# Sorting by the timestamp column didn't work\n",
    "#main_pandas_df.sort_values(by=['@timestamp'])\n",
    "# Convert timestamp to datetime - Use only in case of not running the previous cell\n",
    "main_pandas_df['@timestamp'] = pd.to_datetime(main_pandas_df['@timestamp'])\n",
    "#Set timestamp as the index - the timestamp column will not be deleted\n",
    "main_pandas_df.index = main_pandas_df['@timestamp']\n",
    "#Sort by Timestamp\n",
    "main_pandas_df.sort_index(inplace=True)\n",
    "#Select specific columns to show\n",
    "filtered_result = main_pandas_df[['@timestamp','dest.stats.net_bytes_total','flow_id']]\n",
    "#print(main_filter_id.info())\n",
    "filtered_result.head()\n",
    "\n",
    "# Print first 10 records\n",
    "main_pandas_df.head(10)\n",
    "# filtered = main_pandas_df[(main_pandas_df.type == 'http')]\n",
    "# filtered.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_pandas_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Increase the number of displayed results\n",
    "######################################################################\n",
    "###pd.set_option('display.max_rows', 500)\n",
    "###pd.set_option('display.max_colwidth', -1)\n",
    "######################################################################\n",
    "# Sorting flows by timestamp requires indexing the timestamp column\n",
    "######################################################################\n",
    "# Filtering for a specific flod_id\n",
    "###filter_id = main_pandas_df.flow_id == 'EQQA////DP//////FP8BAAH6Fj4hHCz6Fj5QykYKAQFBCgEBRxriVCQ'\n",
    "###main_filter_id = main_pandas_df[(filter_id)]\n",
    "# Convert timestamp to datetime - Use only in case of not running the previous cell\n",
    "###main_filter_id['@timestamp'] = pd.to_datetime(main_filter_id['@timestamp'])\n",
    "# main_filter_id['@timestamp']#.dt.tz_convert('US/Central')\n",
    "#main_filter_id['@timestamp'].astimezone(timezone('US/Pacific'))\n",
    "# Sorting by the timestamp column didn't work\n",
    "#main_filter_id.sort_values(by=['@timestamp'])\n",
    "#Set timestamp as the index - the timestamp column will not be deleted\n",
    "###main_filter_id.index = main_filter_id['@timestamp']\n",
    "#main_filter_id.index.tz_convert('Europe/Berlin')\n",
    "#Sort by Timestamp\n",
    "###main_filter_id.sort_index(inplace=True)\n",
    "#Select specific columns to show\n",
    "###filtered_result = main_filter_id[['@timestamp','dest.stats.net_bytes_total','flow_id']]\n",
    "#print(main_filter_id.info())\n",
    "###filtered_result\n",
    "#filtered_result2 = main_pandas_df[['@timestamp','dest.stats.net_bytes_total','flow_id']]\n",
    "#filtered_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_pandas_df.to_pickle(\"/mnt/sdb/tcp_udp_dataframe.pkl\")\n",
    "main_pandas_df.to_pickle(\"/mnt/sdb/netflows_2018-10-26_2018-10-30.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = elasticsearch.Elasticsearch([\"149.165.170.209:9200\"])\n",
    "#res = Search(using=es, index=\"filebeat*\").query(\"match\", username=\"root\")\n",
    "res = Search(using=es, index=\"packetbeat*\")\\\n",
    "        .query('bool', filter=Q('exists', field='ip')\\\n",
    "               & Q('exists', field='client_ip')\\\n",
    "               & Q('exists', field='type') & Q('exists', field='status')\\\n",
    "               & Q('exists', field='bytes_in') & Q('exists', field='bytes_out')\\\n",
    "               & Q('match', type='ICMP')\n",
    "              )\n",
    "\n",
    "response = res.execute()\n",
    "\n",
    "print(\"Filtered logs: %i\" %res.count()) #There is an error on the number of matched logs\n",
    "print(\"Total number of logs: %i\" %response.hits.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icmp = pd.DataFrame(((hit[\"@timestamp\"],hit['type'],hit['status'],hit['ip'],hit['client_ip'],hit['bytes_in'],hit['bytes_out']) for hit in res.scan()),\\\n",
    "                    columns=['Timestamp','type','status','ip','client_ip','bytes_in','bytes_out'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icmp.to_pickle(\"icmp_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icmp[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcp_udp[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
